{"cells":[{"cell_type":"markdown","metadata":{"id":"d0W-WuYsjWPP"},"source":["# Artificial Intelligence Course - Fall 1402\n","## Computer Assignment #2 - Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"ksnYjMyNPAcn"},"source":["# Table of Contents\n","\n","- [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n","    - [َQuestion 1:](#1-0)\n","    - [َQuestion 2:](#1-1)\n","    - [َQuestion 3:](#1-12)\n","    - [َQuestion 4:](#1-2)\n","    - [َQuestion 5:](#1-3)\n","        - [Value Iteration](#1-3-1)\n","        - [Policy Iteration](#1-3-2)\n","    - [َQuestion 6:](#1-4)\n","        - [Value Iteration](#1-4-1)\n","        - [Policy Iteration](#1-4-2)\n","- [Part 2: Q-Learning Algorithm](#2)\n","    - [َQuestion 8:](#2-1)\n","    - [َQuestion 9:](#2-2)\n","    - [َQuestion 10:](#2-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpTWKluXMHP5"},"outputs":[],"source":["# import\n","import numpy as np\n","import gym"]},{"cell_type":"markdown","metadata":{"id":"EifP8FUKLXE7"},"source":["<a name='1'></a>\n","## Part 1: Value Iteration & Policy Iteration Algorithms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LizJeOYRMEq"},"outputs":[],"source":["env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZARu6LDSqj_","outputId":"2f22167b-c6ef-493d-bec3-264a93e5d8cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["you can see the environment in each step by render command :\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"]}],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.reset()\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5QTTUSrSM-L","outputId":"206e612c-52ca-49d4-91b8-31f9faf5f1bc"},"outputs":[{"data":{"text/plain":["16"]},"execution_count":217,"metadata":{},"output_type":"execute_result"}],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeaBqUeZSNNY","outputId":"46e8a041-a1e4-4e43-c22a-4e2c1a3a3ddc"},"outputs":[{"data":{"text/plain":["4"]},"execution_count":218,"metadata":{},"output_type":"execute_result"}],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"markdown","source":["<a name='1-0'></a>\n","### Question 1:"],"metadata":{"id":"iVJmGmCUnIGR"}},{"cell_type":"markdown","source":[],"metadata":{"id":"xRDhQMwwnK2s"}},{"cell_type":"markdown","metadata":{"id":"MO24LtBGLXZ7"},"source":["<a name='1-1'></a>\n","### Question 2:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKP8LjK5jGoW"},"outputs":[],"source":["class ValueIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","\n","    def value_estimation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta > self.theta):\n","\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                for action in range(self.env.action_space.n):\n","                    action_value = 0\n","                    for probability, next_state, reward, done in self.env.P[state][action]:\n","                        ### START CODE HERE ###\n","                         action_value += ...\n","                        ### END CODE HERE ###\n","                    self.q_values[state, action] = action_value\n","\n","                self.state_values[state] = np.max(self.q_values[state,:])\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","                if (self.delta < self.theta):\n","                    break\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return np.argmax(self.q_values[state,:])\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","source":["<a name='1-12'></a>\n","### Question 3:"],"metadata":{"id":"frjc5mR4ncm1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"AMJJpf1tnddF"}},{"cell_type":"markdown","metadata":{"id":"V4DcH5yJLXqH"},"source":["<a name='1-2'></a>\n","### Question 4:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"XjSb1lX147hd","executionInfo":{"status":"ok","timestamp":1698433256083,"user_tz":-210,"elapsed":14,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"}}},"outputs":[],"source":["class PolicyIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","        self.policy = np.random.randint(self.env.action_space.n, size=self.env.observation_space.n) # initial policy\n","        self.policy_stable = False\n","\n","    def policy_evaluation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta >= self.theta):\n","\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                new_state_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][self.policy[state]]:\n","                    ### START CODE HERE ###\n","                    new_state_value += ...\n","                    ### END CODE HERE ###\n","                self.state_values[state] = new_state_value\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","    def policy_improvement(self):\n","        self.policy_stable = True\n","\n","        for state in range(self.env.observation_space.n):\n","            old_policy = self.policy[state]\n","\n","            for action in range(self.env.action_space.n):\n","\n","                action_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][action]:\n","                    ### START CODE HERE ###\n","                    action_value += ...\n","                    ### END CODE HERE ###\n","                self.q_values[state, action] = action_value\n","\n","            self.policy[state] = np.argmax(self.q_values[state,:])\n","\n","            if old_policy != self.policy[state]:\n","                self.policy_stable = False\n","\n","    def policy_estimation(self):\n","        self.policy_stable = False\n","\n","        while not self.policy_stable:\n","            self.policy_evaluation()\n","            self.policy_improvement()\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return self.policy[state]\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","metadata":{"id":"u4G-kVjmLYj4"},"source":["<a name='1-3'></a>\n","### Question 5:"]},{"cell_type":"markdown","metadata":{"id":"PB651-ZY4vjE"},"source":["<a name='1-3-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGYLOfYAuKjY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ipZlzoXH40Mn"},"source":["<a name='1-3-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Vxf_xKc44QT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"A3fnAFqJLpVI"},"source":["<a name='1-4'></a>\n","### Question 6:"]},{"cell_type":"markdown","metadata":{"id":"7JuoMPH_PAcv"},"source":["<a name='1-4-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCBnAicAPAcv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C1BMk-BfPAcv"},"source":["<a name='1-4-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnUZpvLP446n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hLtPLm-ELpG9"},"source":["<a name='2'></a>\n","## Part 2: Q-Learning Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cW_rkeDMOE8"},"outputs":[],"source":["# hyperparameters\n","REPS = 20\n","EPISODES = 2000\n","EPSILON = 0.1\n","LEARNING_RATE = 0.1\n","DISCOUNT = 0.9\n","STUDENT_NUM = 123"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1c1w7tRMOR_"},"outputs":[],"source":["# environment\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","Initial_State = env.reset()\n","Initial_State"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou0fWiX_MsZb"},"outputs":[],"source":["taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n","taxi_row, taxi_col, pass_idx, dest_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8tJoWefMOdT"},"outputs":[],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQCB-ZIfNwJM"},"outputs":[],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmk_EYbKNwYT"},"outputs":[],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Et2PlHqMMOoM"},"outputs":[],"source":["# base code for Q-learning\n","\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","\n","\n","for rep in range(REPS):\n","    agent = # Agent Object instance from Algorithm_name(e.g Q_learning_agent) class which has inherited from Agentbase.\n","    for episode in range(EPISODES):\n","        Initial_state = env.reset()\n","\n","        for ... :\n","\n","            bestAction = np.random.choice(ACTIONS)\n","\n","            next_state,rew,done,_ = environment.step(bestAction)\n","\n","            if done:\n","                break"]},{"cell_type":"markdown","metadata":{"id":"oZJAP4nMLpiZ"},"source":["<a name='2-1'></a>\n","### Question 8:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Ue4m5fg9450B","executionInfo":{"status":"ok","timestamp":1698434549406,"user_tz":-210,"elapsed":503,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"}}},"outputs":[],"source":["class QLearningAgent():\n","    def __init__(self, env, epsilon, learning_rate, discount_factor, seed):\n","      self.env = env\n","      self.epsilon = epsilon\n","      self.learning_rate = learning_rate\n","      self.olr = learning_rate\n","      self.discount_factor = discount_factor\n","      self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","      self.seed = seed\n","\n","    def choose_action(self, state):\n","      ### START CODE HERE ###\n","      # With probability epsilon, choose a random action\n","\n","      # Otherwise, choose the action with the highest Q-value\n","\n","      ### END CODE HERE ###\n","      return action\n","\n","    def update_q_table(self, state, action, nextState, reward):\n","      ### START CODE HERE ###\n","      # Calculate the new Q-value using the Q-learning formula\n","      self.q_table[state][action] = ...\n","      ### END CODE HERE ###\n","\n","    def decay_epsilon(self, episode):\n","      ### START CODE HERE ###\n","      self.epsilon = ...\n","      ### END CODE HERE ###\n","\n","    def decrease_learning_rate(self, episode):\n","      ### START CODE HERE ###\n","      self.learning_rate = ...\n","      ### END CODE HERE ###\n","\n","    def take_action(self, action):\n","      next_state, reward, done, _ = self.env.step(action)\n","      return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","      return np.argmax(self.q_table[state])\n","\n","    def get_q_values(self):\n","      return self.q_table\n","\n","    def reset(self):\n","      # self.learning_rate = self.olr\n","      return self.env.reset(seed=self.seed)"]},{"cell_type":"markdown","metadata":{"id":"c5HFAMk-Lpvs"},"source":["<a name='2-2'></a>\n","### Question 9:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgRhXXmwo6MT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BVcMKEGDQWdU"},"source":["<a name='2-3'></a>\n","### Question 10:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlHPV0kqQWdU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}